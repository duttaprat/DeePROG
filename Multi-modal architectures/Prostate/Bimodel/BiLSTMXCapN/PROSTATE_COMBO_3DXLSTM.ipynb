{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from path import Path\n",
    "import torch.utils.data as data\n",
    "from imageio import imread\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from PIL import *\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "    # given query, key,value it finds the rightful weighted component of v to get the attention applied ouput\n",
    "    #q,v,k- batch X length of sequence X features or encoding\n",
    "    #attention sholuld be -batchX7X7\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "#         print(k.transpose(1,2).shape)\n",
    "\n",
    "        attn = torch.bmm(q, k.transpose(1, 2)) \n",
    "#         print(attn.shape)\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "#         print(str(attn.shape)+\" \"+str(v.shape))\n",
    "        output = torch.bmm(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "SDP=ScaledDotProductAttention(5)\n",
    "Ss=SDP(torch.zeros(5,6,100),torch.zeros(5,6,100),torch.zeros(5,6,100))\n",
    "# print(Ss[0].shape)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v)\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "\n",
    "        sz_b, len_q, _ = q.size()\n",
    "        sz_b, len_k, _ = k.size()\n",
    "        sz_b, len_v, _ = v.size()\n",
    "#         print(str(sz_b)+\"die\")\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k) # (n*b) x lq x dk\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k) # (n*b) x lk x dk\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v) # (n*b) x lv x dv\n",
    "#         print(\"v-\"+str(v.shape))\n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1) # (n*b) x .. x ..\n",
    "        output, attn = self.attention(q, k, v, mask=mask)\n",
    "#         print(q.shape,k.shape,v.shape)\n",
    "        output = output.view(n_head, sz_b, len_q, d_v)\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1) # b x lq x (n*dv)\n",
    "\n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output, attn\n",
    "MHA=MultiHeadAttention(4,15,15,15)\n",
    "op=MHA(torch.zeros(5,7,15),torch.zeros(5,7,15),torch.zeros(5,7,15))\n",
    "# print(op[0].shape)\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Conv1d(d_in, d_hid, 1) # position-wise\n",
    "        self.w_2 = nn.Conv1d(d_hid, d_in, 1) # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = x.transpose(1, 2)\n",
    "#         print(\"FCC-\"+str(output.shape))\n",
    "#         print(\"FFC_out-\"+str(self.w_1(output).shape))\n",
    "        output = self.w_2(F.relu(self.w_1(output)))\n",
    "        output = output.transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    ''' Compose with two layers '''\n",
    "\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(\n",
    "            n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, non_pad_mask=None, slf_attn_mask=None):\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
    "#         print(\"ENC_o\")\n",
    "#         print(enc_output.shape)\n",
    "\n",
    "\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "\n",
    "\n",
    "        return enc_output, enc_slf_attn\n",
    "    \n",
    "XX=EncoderLayer(15,10,4,10,10)\n",
    "\n",
    "zz=XX(torch.zeros(5,7,15))\n",
    "# print(\"ENc\")\n",
    "# print(zz[0].shape)\n",
    "# print(\"start\")\n",
    "class Encoder(nn.Module):\n",
    "    ''' A encoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(self,n_modality,d_model,n_head,d_k,d_v,dropout,n_layers,d_inner=500):\n",
    "        #d_model - number of features in input 100 here\n",
    "        #n_head - number of heads of multihaded attention\n",
    "        #d_k=d_q=  number of features in query, key\n",
    "        #d_v = number of features in value whose weighted(attentioned) sum we gonna take\n",
    "        \n",
    "\n",
    "        super().__init__()\n",
    "        self.n_modality=n_modality\n",
    "#         self.stn=nn.ModuleList([SpatialTransformer(3, (240,240), 8) for _ in range(n_ref)])\n",
    "        \n",
    "        self.layer_stack = nn.ModuleList([EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout) \n",
    "                                          for _ in range(n_layers)])\n",
    "        self.em=nn.Linear(32,100)\n",
    "        self.fc1=nn.Linear(d_model*n_modality,300)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.fc2=nn.Linear(300,100)\n",
    "        self.fc3=nn.Linear(100,2)\n",
    "#         self.fc4=nn.Linear(50,3)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=300)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=100)\n",
    "        self.softmax=nn.Softmax(1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embeddings1,embeddings2 ):\n",
    "\n",
    "\n",
    "        \n",
    "        encodings_total=[embeddings1,self.em(embeddings2)]\n",
    "\n",
    "        enc_output=torch.stack(encodings_total,0)\n",
    "\n",
    "        \n",
    "        enc_output=enc_output.permute(1,0,2)\n",
    "#         print(\"encoding_OUTPUT2-\"+str(enc_output.shape))\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(enc_output,non_pad_mask=None,slf_attn_mask=None)\n",
    "           \n",
    "        \n",
    "        final_input=enc_output.reshape(enc_output.shape[0],-1)\n",
    "\n",
    "        final=self.relu(self.fc3(self.bn2(self.relu((self.fc2(self.bn1(self.relu(self.fc1(final_input)))))))))\n",
    "        \n",
    "        return(final)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1=torch.zeros(4,100)\n",
    "e2=torch.zeros(4,100)\n",
    "e3=torch.stack([e1,e2],0)\n",
    "e3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_name</th>\n",
       "      <th>tag_0</th>\n",
       "      <th>tag_1</th>\n",
       "      <th>tag_2</th>\n",
       "      <th>tag_3</th>\n",
       "      <th>tag_4</th>\n",
       "      <th>tag_5</th>\n",
       "      <th>tag_6</th>\n",
       "      <th>tag_7</th>\n",
       "      <th>tag_8</th>\n",
       "      <th>...</th>\n",
       "      <th>tag_22</th>\n",
       "      <th>tag_23</th>\n",
       "      <th>tag_24</th>\n",
       "      <th>tag_25</th>\n",
       "      <th>tag_26</th>\n",
       "      <th>tag_27</th>\n",
       "      <th>tag_28</th>\n",
       "      <th>tag_29</th>\n",
       "      <th>tag_30</th>\n",
       "      <th>tag_31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhl</td>\n",
       "      <td>-0.004627</td>\n",
       "      <td>0.245317</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>0.069272</td>\n",
       "      <td>-0.003327</td>\n",
       "      <td>-0.263032</td>\n",
       "      <td>-0.003749</td>\n",
       "      <td>0.251758</td>\n",
       "      <td>-0.003560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>-0.229637</td>\n",
       "      <td>0.004711</td>\n",
       "      <td>0.266983</td>\n",
       "      <td>-0.003079</td>\n",
       "      <td>0.228596</td>\n",
       "      <td>0.003680</td>\n",
       "      <td>-0.258568</td>\n",
       "      <td>-0.003935</td>\n",
       "      <td>0.195633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ndufa10</td>\n",
       "      <td>0.275199</td>\n",
       "      <td>0.009126</td>\n",
       "      <td>-0.289780</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>0.173320</td>\n",
       "      <td>-0.012266</td>\n",
       "      <td>0.259847</td>\n",
       "      <td>0.010369</td>\n",
       "      <td>0.283457</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258688</td>\n",
       "      <td>-0.011406</td>\n",
       "      <td>-0.293413</td>\n",
       "      <td>0.009331</td>\n",
       "      <td>0.251243</td>\n",
       "      <td>0.009058</td>\n",
       "      <td>-0.211729</td>\n",
       "      <td>-0.013103</td>\n",
       "      <td>0.264524</td>\n",
       "      <td>0.007954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ube4b</td>\n",
       "      <td>0.290513</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>-0.282832</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.173428</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>0.258021</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>0.284236</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.266636</td>\n",
       "      <td>-0.000551</td>\n",
       "      <td>-0.294954</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.244364</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>-0.203645</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>0.275976</td>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>foxo3</td>\n",
       "      <td>0.281134</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>-0.286166</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.173116</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.261762</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.288521</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.266013</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.291289</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.253075</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>-0.203884</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>0.268459</td>\n",
       "      <td>-0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rap2a</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.245117</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.066403</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.267965</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.249047</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>-0.223775</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.271736</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>0.235593</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-0.267288</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.193028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  gene_name     tag_0     tag_1     tag_2     tag_3     tag_4     tag_5  \\\n",
       "0       vhl -0.004627  0.245317  0.004466  0.069272 -0.003327 -0.263032   \n",
       "1   ndufa10  0.275199  0.009126 -0.289780  0.004290  0.173320 -0.012266   \n",
       "2     ube4b  0.290513  0.000389 -0.282832  0.000397  0.173428 -0.000026   \n",
       "3     foxo3  0.281134  0.000155 -0.286166  0.000090  0.173116 -0.000050   \n",
       "4     rap2a  0.000149  0.245117 -0.000038  0.066403 -0.000030 -0.267965   \n",
       "\n",
       "      tag_6     tag_7     tag_8  ...    tag_22    tag_23    tag_24    tag_25  \\\n",
       "0 -0.003749  0.251758 -0.003560  ...  0.003328 -0.229637  0.004711  0.266983   \n",
       "1  0.259847  0.010369  0.283457  ... -0.258688 -0.011406 -0.293413  0.009331   \n",
       "2  0.258021 -0.000054  0.284236  ... -0.266636 -0.000551 -0.294954  0.000467   \n",
       "3  0.261762  0.000328  0.288521  ... -0.266013 -0.000130 -0.291289  0.000215   \n",
       "4 -0.000021  0.249047  0.000241  ... -0.000095 -0.223775  0.000131  0.271736   \n",
       "\n",
       "     tag_26    tag_27    tag_28    tag_29    tag_30    tag_31  \n",
       "0 -0.003079  0.228596  0.003680 -0.258568 -0.003935  0.195633  \n",
       "1  0.251243  0.009058 -0.211729 -0.013103  0.264524  0.007954  \n",
       "2  0.244364  0.000293 -0.203645 -0.000146  0.275976  0.000081  \n",
       "3  0.253075  0.000154 -0.203884 -0.000096  0.268459 -0.000015  \n",
       "4 -0.000234  0.235593  0.000200 -0.267288  0.000179  0.193028  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set_MLP=pd.read_csv('Prostrate_MLP_features_ankit.csv')\n",
    "feature_set_3d=pd.read_csv('Ankitprostate_3d.csv')\n",
    "feature_set_LSTM=pd.read_csv('prostrate_LSTM_features_ankit.csv')\n",
    "feature_set_3d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_name</th>\n",
       "      <th>tag_0</th>\n",
       "      <th>tag_1</th>\n",
       "      <th>tag_2</th>\n",
       "      <th>tag_3</th>\n",
       "      <th>tag_4</th>\n",
       "      <th>tag_5</th>\n",
       "      <th>tag_6</th>\n",
       "      <th>tag_7</th>\n",
       "      <th>tag_8</th>\n",
       "      <th>...</th>\n",
       "      <th>tag_90</th>\n",
       "      <th>tag_91</th>\n",
       "      <th>tag_92</th>\n",
       "      <th>tag_93</th>\n",
       "      <th>tag_94</th>\n",
       "      <th>tag_95</th>\n",
       "      <th>tag_96</th>\n",
       "      <th>tag_97</th>\n",
       "      <th>tag_98</th>\n",
       "      <th>tag_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhl</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.241405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.266189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smox</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.911898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.720570</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>znf148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.678095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.640351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>map4k2</td>\n",
       "      <td>20.404165</td>\n",
       "      <td>23.852463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.247234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.033705</td>\n",
       "      <td>0.203660</td>\n",
       "      <td>56.684559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.108454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.755461</td>\n",
       "      <td>5.603198</td>\n",
       "      <td>13.697145</td>\n",
       "      <td>63.017044</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mapk4</td>\n",
       "      <td>42.203545</td>\n",
       "      <td>52.076824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.526337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.578827</td>\n",
       "      <td>10.026752</td>\n",
       "      <td>108.900192</td>\n",
       "      <td>...</td>\n",
       "      <td>3.838364</td>\n",
       "      <td>40.393089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.628872</td>\n",
       "      <td>18.609310</td>\n",
       "      <td>32.895622</td>\n",
       "      <td>120.597061</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  gene_name      tag_0      tag_1  tag_2      tag_3      tag_4  tag_5  \\\n",
       "0       vhl   0.000000   0.000000    0.0  22.241405   0.000000    0.0   \n",
       "1      smox   0.000000   0.000000    0.0   0.000000   0.000000    0.0   \n",
       "2    znf148   0.000000   0.000000    0.0  21.678095   0.000000    0.0   \n",
       "3    map4k2  20.404165  23.852463    0.0   0.000000  43.247234    0.0   \n",
       "4     mapk4  42.203545  52.076824    0.0   0.000000  89.526337    0.0   \n",
       "\n",
       "       tag_6      tag_7       tag_8  ...    tag_90     tag_91  tag_92  tag_93  \\\n",
       "0   0.000000   0.000000    0.000000  ...  0.000000   0.000000     0.0     0.0   \n",
       "1   0.000000   0.000000    7.911898  ...  0.000000   0.000000     0.0     0.0   \n",
       "2   0.000000   0.000000    0.000000  ...  0.000000   0.000000     0.0     0.0   \n",
       "3  31.033705   0.203660   56.684559  ...  0.000000  18.108454     0.0     0.0   \n",
       "4  67.578827  10.026752  108.900192  ...  3.838364  40.393089     0.0     0.0   \n",
       "\n",
       "   tag_94     tag_95     tag_96     tag_97      tag_98     tag_99  \n",
       "0     0.0   0.000000   0.000000   0.000000    0.000000  10.266189  \n",
       "1     0.0   0.000000   0.000000   0.000000   12.720570   0.000000  \n",
       "2     0.0   0.000000   0.000000   0.000000    0.000000  10.640351  \n",
       "3     0.0  28.755461   5.603198  13.697145   63.017044   0.000000  \n",
       "4     0.0  57.628872  18.609310  32.895622  120.597061   0.000000  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set_MLP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2424, 100)\n",
      "2424\n"
     ]
    }
   ],
   "source": [
    "header_of_MLP=['tag_'+str(i) for i in range(feature_set_MLP.shape[1]-1)]\n",
    "features_MLP=np.array(feature_set_MLP[header_of_MLP])\n",
    "gene_MLP=feature_set_MLP['gene_name']\n",
    "print(features_MLP.shape)\n",
    "print(len(gene_MLP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_MLP={}\n",
    "u=0\n",
    "for gn in gene_MLP:\n",
    "    dictionary_MLP[gn]=features_MLP[u]\n",
    "    u=u+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(723, 32)\n",
      "723\n"
     ]
    }
   ],
   "source": [
    "header_of_3d=['tag_'+str(i) for i in range(feature_set_3d.shape[1]-1)]\n",
    "features_3d=np.array(feature_set_3d[header_of_3d])\n",
    "gene_3d=feature_set_3d['gene_name']\n",
    "print(features_3d.shape)\n",
    "print(len(gene_3d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723\n"
     ]
    }
   ],
   "source": [
    "dictionary_3d={}\n",
    "u=f=0\n",
    "for gn in gene_3d:\n",
    "    if gn in dictionary_3d.keys():\n",
    "#         print(gn)\n",
    "        f=f+1\n",
    "    dictionary_3d[gn]=features_3d[u]\n",
    "    u=u+1\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary_3d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1763, 100)\n",
      "1763\n"
     ]
    }
   ],
   "source": [
    "header_of_lstm=['tag_'+str(i) for i in range(feature_set_LSTM.shape[1]-1)]\n",
    "features_lstm=np.array(feature_set_LSTM[header_of_lstm])\n",
    "gene_lstm=feature_set_LSTM['gene_name']\n",
    "print(features_lstm.shape)\n",
    "print(len(gene_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1763\n"
     ]
    }
   ],
   "source": [
    "dictionary_lstm={}\n",
    "u=f=0\n",
    "for gn in gene_lstm:\n",
    "    if gn in dictionary_lstm.keys():\n",
    "#         print(gn)\n",
    "        f=f+1\n",
    "    dictionary_lstm[gn]=features_lstm[u]\n",
    "    u=u+1\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2424,)\n",
      "2424\n"
     ]
    }
   ],
   "source": [
    "fil=open('../../Multi-modality/Model/Prostrate/uni model/MLP/Labels_prostate.txt','r')\n",
    "tmp=list()\n",
    "for line in fil:\n",
    "\ttmp.append(int(line))\n",
    "\n",
    "label_PROSTATE=np.array(tmp)\n",
    "print(label_PROSTATE.shape)\n",
    "print(len(gene_MLP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309 414 0\n"
     ]
    }
   ],
   "source": [
    "class Sequenceloader(data.Dataset):\n",
    "    def __init__(self,GN,label):\n",
    "        self.gene_names=GN\n",
    "#         self.features_lstm=Feat\n",
    "        self.label=label\n",
    "        self.coincdgene_name=[]\n",
    "        self.coincidfeature_lstm=[]\n",
    "        self.coincidfeature_3d=[]\n",
    "        self.coincidlabel=[]\n",
    "        for i in range(len(self.gene_names)):\n",
    "            u=self.gene_names[i]\n",
    "            if u in dictionary_3d.keys() and u in dictionary_lstm.keys():\n",
    "                \n",
    "                if np.array(self.label[i])==2:\n",
    "                    ch=3\n",
    "                else:\n",
    "                    ch=1\n",
    "                    \n",
    "                for jj in range(ch):\n",
    "                    self.coincdgene_name.append(u)\n",
    "                    self.coincidfeature_lstm.append(dictionary_lstm[u])\n",
    "                    self.coincidfeature_3d.append(dictionary_3d[u])\n",
    "                    self.coincidlabel.append(self.label[i])\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "#         print(len(self.dataset))\n",
    "        return len(self.coincdgene_name)       \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "         return np.array(self.coincidfeature_lstm[index]),np.array(self.coincidfeature_3d[index]),np.array(self.coincidlabel[index])\n",
    "#         print(self.dataset['gen_name'][index])          \n",
    "#         try:\n",
    "           \n",
    "#         except :\n",
    "            \n",
    "total_set=Sequenceloader(gene_MLP,label_PROSTATE)  \n",
    "a=b=c=0\n",
    "for x,y,z in total_set:\n",
    "    if(z==0):\n",
    "        a=a+1\n",
    "    elif z==1:\n",
    "        b=b+1\n",
    "    else:\n",
    "        c=c+1\n",
    "print(a,b,c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(total_set)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(total_set, batch_size=batch_size, \n",
    "                                           sampler=train_sampler,drop_last=True)\n",
    "validation_loader = torch.utils.data.DataLoader(total_set, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n",
      "torch.Size([4, 100]) torch.Size([4, 32]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for a,b,c in train_loader:\n",
    "    print(a.shape,b.shape,c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader,model):\n",
    "\n",
    "    total_imgs=0;\n",
    "    total_corrects=0\n",
    "    u=0\n",
    "    nb_classes=2\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    for i1,i2,label in test_loader:\n",
    "                \n",
    "\n",
    "        output=model(i1.to(device).float(),i2.to(device).float())\n",
    "        total_imgs=total_imgs+label.shape[0]\n",
    "        z=torch.max(output,1)[1]==label.to(device)\n",
    "        _, preds = torch.max(output, 1)\n",
    "#         print(output.shape)\n",
    " \n",
    "        num_corrects=torch.sum(z)\n",
    "        total_corrects=total_corrects+num_corrects\n",
    "        for t, p in zip(label.view(-1), preds.view(-1)):\n",
    "            confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "\n",
    "        u=u+1\n",
    "    \n",
    "    \n",
    "    print(confusion_matrix)\n",
    "    return(total_corrects,total_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:7'\n",
    "model_3dXLSTM=Encoder(2,100,6,300,300,True,4).to(device)\n",
    "#model_3dXLSTM.load_state_dict(torch.load(Path('2AnkitILD COMBO_3DXLSTM.pt')))\n",
    "uz=vz=torch.rand(4,100).to(device)\n",
    "# model_3dXMLP(uz,vz).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.load_state_dict(torch.load(Path('currentz_m1_epoch.pt')))\n",
    "optim_params = [\n",
    "    {'params': model_3dXLSTM.parameters(), 'lr': 0.0001}\n",
    "]\n",
    "optimizer = torch.optim.Adam(optim_params)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[47., 16.],\n",
      "        [20., 61.]])\n",
      "Accuracy-(tensor(108, device='cuda:7'), 144)\n",
      "tensor(11.2378, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[46., 17.],\n",
      "        [19., 62.]])\n",
      "Accuracy-(tensor(108, device='cuda:7'), 144)\n",
      "tensor(11.6701, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[48., 15.],\n",
      "        [19., 62.]])\n",
      "Accuracy-(tensor(110, device='cuda:7'), 144)\n",
      "tensor(11.9803, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[45., 18.],\n",
      "        [22., 59.]])\n",
      "Accuracy-(tensor(104, device='cuda:7'), 144)\n",
      "tensor(10.5869, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[42., 21.],\n",
      "        [20., 61.]])\n",
      "Accuracy-(tensor(103, device='cuda:7'), 144)\n",
      "tensor(11.5178, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[47., 16.],\n",
      "        [19., 62.]])\n",
      "Accuracy-(tensor(109, device='cuda:7'), 144)\n",
      "tensor(10.9128, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[45., 18.],\n",
      "        [15., 66.]])\n",
      "Accuracy-(tensor(111, device='cuda:7'), 144)\n",
      "tensor(10.9582, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[47., 16.],\n",
      "        [24., 57.]])\n",
      "Accuracy-(tensor(104, device='cuda:7'), 144)\n",
      "tensor(10.7234, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[45., 18.],\n",
      "        [18., 63.]])\n",
      "Accuracy-(tensor(108, device='cuda:7'), 144)\n",
      "tensor(10.6564, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[43., 20.],\n",
      "        [16., 65.]])\n",
      "Accuracy-(tensor(108, device='cuda:7'), 144)\n",
      "tensor(10.7652, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[44., 19.],\n",
      "        [17., 64.]])\n",
      "Accuracy-(tensor(108, device='cuda:7'), 144)\n",
      "tensor(11.6498, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[38., 25.],\n",
      "        [21., 60.]])\n",
      "Accuracy-(tensor(98, device='cuda:7'), 144)\n",
      "tensor(10.4411, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[45., 18.],\n",
      "        [18., 63.]])\n",
      "Accuracy-(tensor(108, device='cuda:7'), 144)\n",
      "tensor(10.5973, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[47., 16.],\n",
      "        [15., 66.]])\n",
      "Accuracy-(tensor(113, device='cuda:7'), 144)\n",
      "tensor(11.9801, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[50., 13.],\n",
      "        [17., 64.]])\n",
      "Accuracy-(tensor(114, device='cuda:7'), 144)\n",
      "tensor(11.4280, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[47., 16.],\n",
      "        [15., 66.]])\n",
      "Accuracy-(tensor(113, device='cuda:7'), 144)\n",
      "tensor(10.8308, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[48., 15.],\n",
      "        [20., 61.]])\n",
      "Accuracy-(tensor(109, device='cuda:7'), 144)\n",
      "tensor(10.8293, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[47., 16.],\n",
      "        [20., 61.]])\n",
      "Accuracy-(tensor(108, device='cuda:7'), 144)\n",
      "tensor(11.0541, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[50., 13.],\n",
      "        [21., 60.]])\n",
      "Accuracy-(tensor(110, device='cuda:7'), 144)\n",
      "tensor(10.9903, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[46., 17.],\n",
      "        [21., 60.]])\n",
      "Accuracy-(tensor(106, device='cuda:7'), 144)\n",
      "tensor(12.2063, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[45., 18.],\n",
      "        [17., 64.]])\n",
      "Accuracy-(tensor(109, device='cuda:7'), 144)\n",
      "tensor(10.7548, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[43., 20.],\n",
      "        [24., 57.]])\n",
      "Accuracy-(tensor(100, device='cuda:7'), 144)\n",
      "tensor(11.4364, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[44., 19.],\n",
      "        [16., 65.]])\n",
      "Accuracy-(tensor(109, device='cuda:7'), 144)\n",
      "tensor(12.4458, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[45., 18.],\n",
      "        [20., 61.]])\n",
      "Accuracy-(tensor(106, device='cuda:7'), 144)\n",
      "tensor(11.3839, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[46., 17.],\n",
      "        [18., 63.]])\n",
      "Accuracy-(tensor(109, device='cuda:7'), 144)\n",
      "tensor(11.4442, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[48., 15.],\n",
      "        [18., 63.]])\n",
      "Accuracy-(tensor(111, device='cuda:7'), 144)\n",
      "tensor(10.6013, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[47., 16.],\n",
      "        [16., 65.]])\n",
      "Accuracy-(tensor(112, device='cuda:7'), 144)\n",
      "tensor(10.7197, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[48., 15.],\n",
      "        [16., 65.]])\n",
      "Accuracy-(tensor(113, device='cuda:7'), 144)\n",
      "tensor(11.3238, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[47., 16.],\n",
      "        [18., 63.]])\n",
      "Accuracy-(tensor(110, device='cuda:7'), 144)\n",
      "tensor(11.0981, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[47., 16.],\n",
      "        [15., 66.]])\n",
      "Accuracy-(tensor(113, device='cuda:7'), 144)\n",
      "tensor(10.4742, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[46., 17.],\n",
      "        [16., 65.]])\n",
      "Accuracy-(tensor(111, device='cuda:7'), 144)\n",
      "tensor(11.2930, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[46., 17.],\n",
      "        [14., 67.]])\n",
      "Accuracy-(tensor(113, device='cuda:7'), 144)\n",
      "tensor(11.8025, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[45., 18.],\n",
      "        [18., 63.]])\n",
      "Accuracy-(tensor(108, device='cuda:7'), 144)\n",
      "tensor(11.2948, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[43., 20.],\n",
      "        [21., 60.]])\n",
      "Accuracy-(tensor(103, device='cuda:7'), 144)\n",
      "tensor(11.0936, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[45., 18.],\n",
      "        [19., 62.]])\n",
      "Accuracy-(tensor(107, device='cuda:7'), 144)\n",
      "tensor(11.3242, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[47., 16.],\n",
      "        [22., 59.]])\n",
      "Accuracy-(tensor(106, device='cuda:7'), 144)\n",
      "tensor(11.4233, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[45., 18.],\n",
      "        [20., 61.]])\n",
      "Accuracy-(tensor(106, device='cuda:7'), 144)\n",
      "tensor(11.2887, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[41., 22.],\n",
      "        [18., 63.]])\n",
      "Accuracy-(tensor(104, device='cuda:7'), 144)\n",
      "tensor(11.0177, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[48., 15.],\n",
      "        [17., 64.]])\n",
      "Accuracy-(tensor(112, device='cuda:7'), 144)\n",
      "tensor(10.9921, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[48., 15.],\n",
      "        [19., 62.]])\n",
      "Accuracy-(tensor(110, device='cuda:7'), 144)\n",
      "tensor(10.9755, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[44., 19.],\n",
      "        [20., 61.]])\n",
      "Accuracy-(tensor(105, device='cuda:7'), 144)\n",
      "tensor(10.0856, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[49., 14.],\n",
      "        [15., 66.]])\n",
      "Accuracy-(tensor(115, device='cuda:7'), 144)\n",
      "tensor(10.4720, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[47., 16.],\n",
      "        [13., 68.]])\n",
      "Accuracy-(tensor(115, device='cuda:7'), 144)\n",
      "tensor(11.1020, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[44., 19.],\n",
      "        [16., 65.]])\n",
      "Accuracy-(tensor(109, device='cuda:7'), 144)\n",
      "tensor(10.4868, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[49., 14.],\n",
      "        [17., 64.]])\n",
      "Accuracy-(tensor(113, device='cuda:7'), 144)\n",
      "tensor(9.9296, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[45., 18.],\n",
      "        [15., 66.]])\n",
      "Accuracy-(tensor(111, device='cuda:7'), 144)\n",
      "tensor(10.3345, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[43., 20.],\n",
      "        [17., 64.]])\n",
      "Accuracy-(tensor(107, device='cuda:7'), 144)\n",
      "tensor(11.6384, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[46., 17.],\n",
      "        [16., 65.]])\n",
      "Accuracy-(tensor(111, device='cuda:7'), 144)\n",
      "tensor(11.7183, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[49., 14.],\n",
      "        [20., 61.]])\n",
      "Accuracy-(tensor(110, device='cuda:7'), 144)\n",
      "tensor(10.1035, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[46., 17.],\n",
      "        [17., 64.]])\n",
      "Accuracy-(tensor(110, device='cuda:7'), 144)\n",
      "tensor(11.2501, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[43., 20.],\n",
      "        [16., 65.]])\n",
      "Accuracy-(tensor(108, device='cuda:7'), 144)\n",
      "tensor(11.0005, device='cuda:7', grad_fn=<AddBackward0>)\n",
      "tensor([[53., 10.],\n",
      "        [19., 62.]])\n",
      "Accuracy-(tensor(115, device='cuda:7'), 144)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-4ad0c5e38294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_3dXLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch=10000\n",
    "for i in range(epoch):\n",
    "    torch.save(model_3dXLSTM.state_dict(), '2Ankit_PROSTATE_COMBO_3DXLSTM.pt')\n",
    "    print(\"Accuracy-\"+str(test(validation_loader,model_3dXLSTM)))\n",
    "    total_loss=0\n",
    "    for inp1,inp2,lab in train_loader:\n",
    "\n",
    "        \n",
    "\n",
    "        output=model_3dXLSTM(inp1.to(device).float(),inp2.to(device).float())\n",
    "\n",
    "        loss_batch=criterion(output,lab.to(device))/4\n",
    "        optimizer.zero_grad()  \n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss_batch\n",
    "       \n",
    "    print(total_loss)\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "tensor([[53., 10.],\n",
    "        [19., 62.]])\n",
    "Accuracy-(tensor(115, device='cuda:7'), 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
